{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# เรียกใช้งานโมดูล\n",
    "file_name=\"data\"\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "#import deepcut\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "# thai cut\n",
    "thaicut=\"newmm\"\n",
    "from sklearn_crfsuite import scorers,metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate,train_test_split\n",
    "import sklearn_crfsuite\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "stopwords = list(thai_stopwords())\n",
    "#จัดการประโยคซ้ำ\n",
    "data_not=[]\n",
    "def Unique(p):\n",
    " text=re.sub(\"<[^>]*>\",\"\",p)\n",
    " text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    " text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    " if text not in data_not:\n",
    "  data_not.append(text)\n",
    "  return True\n",
    " else:\n",
    "  return False\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    " text=text.strip().replace(\"FACILITY\",\"LOCATION\").replace(\"[AGO]\",\"\").replace(\"[/AGO]\",\"\").replace(\"[T]\",\"\").replace(\"[/T]\",\"\")\n",
    " text=re.sub(\"<[^>]*>\",\"\",text)\n",
    " text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')# text.replace('>','>***') # ตัดการกับพวกไม่มี tag word\n",
    " text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    " text2=[]\n",
    " for i in text.split('***'):\n",
    "  if \"[\" in i:\n",
    "   text2.append(i)\n",
    "  else:\n",
    "   text2.append(\"[word]\"+i+\"[/word]\")\n",
    " text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    " return text.replace(\"[word][/word]\",\"\")\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text)\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text)\n",
    "    j=0\n",
    "    conll2002=\"\"\n",
    "    for tagopen,text,tagclose in tag:\n",
    "        word_cut=word_tokenize(text,engine=thaicut) # ใช้ตัวตัดคำ newmm\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut):\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen\n",
    "            else:\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n'\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002)\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\", corpus=\"orchid_ud\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'\n",
    "        i+=1\n",
    "    return text\n",
    "# เขียนไฟล์ข้อมูล conll2002\n",
    "def write_conll2002(file_name,data):\n",
    "    \"\"\"\n",
    "    ใช้สำหรับเขียนไฟล์\n",
    "    \"\"\"\n",
    "    with codecs.open(file_name, \"w\", \"utf-8-sig\") as temp:\n",
    "        temp.write(data)\n",
    "    return True\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "\t\"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "\twith codecs.open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\treturn [a for a in tqdm(lines) if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def alldata(lists):\n",
    "    text=\"\"\n",
    "    for data in lists:\n",
    "        text+=text2conll2002(data)\n",
    "        text+='\\n'\n",
    "    return text\n",
    "\n",
    "def alldata_list(lists):\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=True).split('\\n')\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2]))\n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "    #print(data_all)\n",
    "    return data_all\n",
    "\n",
    "def alldata_list_str(lists):\n",
    "\tstring=\"\"\n",
    "\tfor data in lists:\n",
    "\t\tstring1=\"\"\n",
    "\t\tfor j in data:\n",
    "\t\t\tstring1+=j[0]+\"\t\"+j[1]+\"\t\"+j[2]+\"\\n\"\n",
    "\t\tstring1+=\"\\n\"\n",
    "\t\tstring+=string1\n",
    "\treturn string\n",
    "\n",
    "def get_data_tag(listd):\n",
    "\tlist_all=[]\n",
    "\tc=[]\n",
    "\tfor i in listd:\n",
    "\t\tif i !='':\n",
    "\t\t\tc.append((i.split(\"\\t\")[0],i.split(\"\\t\")[1],i.split(\"\\t\")[2]))\n",
    "\t\telse:\n",
    "\t\t\tlist_all.append(c)\n",
    "\t\t\tc=[]\n",
    "\treturn list_all\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in tqdm(lista):\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6457/6457 [00:00<00:00, 12023.70it/s]\n",
      "100%|██████████| 6349/6349 [03:18<00:00, 20.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6349\n",
      "6349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'training_samples = datatofile[:2822]\\ntest_samples = datatofile[2822:]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=getall(get_data(file_name+\".txt\"))\n",
    "print(len(data1))\n",
    "'''\n",
    "'''\n",
    "#del datatofile[0]\n",
    "datatofile=alldata_list(data1)\n",
    "tt=[]\n",
    "#datatofile.reverse()\n",
    "import random\n",
    "#random.shuffle(datatofile)\n",
    "print(len(datatofile))\n",
    "#training_samples = datatofile[:int(len(datatofile) * 0.8)]\n",
    "#test_samples = datatofile[int(len(datatofile) * 0.8):]\n",
    "'''training_samples = datatofile[:2822]\n",
    "test_samples = datatofile[2822:]'''\n",
    "#print(test_samples[0])\n",
    "#tag=TrainChunker(training_samples,test_samples) # Train\n",
    "\n",
    "#run(training_samples,test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dill\n",
    "#with open('train.data', 'rb') as file:\n",
    "# datatofile = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name+\"-pos.conll\",\"w\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile):\n",
    "        for j in datatofile[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1\n",
    "\n",
    "with open(file_name+\".conll\",\"w\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile):\n",
    "        for j in datatofile[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isThai(chr):\n",
    " cVal = ord(chr)\n",
    " if(cVal >= 3584 and cVal <= 3711):\n",
    "  return True\n",
    " return False\n",
    "def isThaiWord(word):\n",
    " t=True\n",
    " for i in word:\n",
    "  l=isThai(i)\n",
    "  if l!=True and i!='.':\n",
    "   t=False\n",
    "   break\n",
    " return t\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word in stopwords\n",
    "def is_s(word):\n",
    "    if word == \" \" or word ==\"\\t\" or word==\"\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def lennum(word,num):\n",
    "    if len(word)==num:\n",
    "        return True\n",
    "    return False\n",
    "def doc2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "    # Features from current word\n",
    "    features={\n",
    "        'word.word': word,\n",
    "        'word.stopword': is_stopword(word),\n",
    "        'word.isthai':isThaiWord(word),\n",
    "        'word.isspace':word.isspace(),\n",
    "        'postag':postag,\n",
    "        'word.isdigit()': word.isdigit()\n",
    "    }\n",
    "    if word.isdigit() and len(word)==5:\n",
    "        features['word.islen5']=True\n",
    "    if i > 0:\n",
    "        prevword = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features['word.prevword'] = prevword\n",
    "        features['word.previsspace']=prevword.isspace()\n",
    "        features['word.previsthai']=isThaiWord(prevword)\n",
    "        features['word.prevstopword']=is_stopword(prevword)\n",
    "        features['word.prepostag'] = postag1\n",
    "        features['word.prevwordisdigit'] = prevword.isdigit()\n",
    "    else:\n",
    "        features['BOS'] = True # Special \"Beginning of Sequence\" tag\n",
    "    # Features from next word\n",
    "    if i < len(doc)-1:\n",
    "        nextword = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features['word.nextword'] = nextword\n",
    "        features['word.nextisspace']=nextword.isspace()\n",
    "        features['word.nextpostag'] = postag1\n",
    "        features['word.nextisthai']=isThaiWord(nextword)\n",
    "        features['word.nextstopword']=is_stopword(nextword)\n",
    "        features['word.nextwordisdigit'] = nextword.isdigit()\n",
    "    else:\n",
    "        features['EOS'] = True # Special \"End of Sequence\" tag\n",
    "    return features\n",
    "\n",
    "def extract_features(doc):\n",
    "    return [doc2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def get_labels(doc):\n",
    "    return [tag for (token,postag,tag) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6349/6349 [00:11<00:00, 549.11it/s]\n",
      "100%|██████████| 6349/6349 [00:00<00:00, 214278.19it/s]\n"
     ]
    }
   ],
   "source": [
    "X_data = [extract_features(doc) for doc in tqdm(datatofile)]\n",
    "y_data = [get_labels(doc) for doc in tqdm(datatofile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wannaphong/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/wannaphong/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8678830255745219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wannaphong/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/wannaphong/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "        B-DATE      0.945     0.879     0.911       389\n",
      "        I-DATE      0.971     0.937     0.954       759\n",
      "       B-EMAIL      0.000     0.000     0.000         0\n",
      "       I-EMAIL      0.000     0.000     0.000         0\n",
      "         B-LAW      0.789     0.600     0.682        50\n",
      "         I-LAW      0.735     0.704     0.719       142\n",
      "         B-LEN      0.967     0.725     0.829        40\n",
      "         I-LEN      0.971     0.720     0.827        93\n",
      "    B-LOCATION      0.890     0.736     0.805       965\n",
      "    I-LOCATION      0.872     0.735     0.797      1018\n",
      "       B-MONEY      0.972     0.875     0.921       120\n",
      "       I-MONEY      0.971     0.927     0.948       288\n",
      "B-ORGANIZATION      0.897     0.767     0.827      1133\n",
      "I-ORGANIZATION      0.816     0.731     0.771      1352\n",
      "     B-PERCENT      0.892     0.892     0.892        37\n",
      "     I-PERCENT      0.927     0.911     0.919        56\n",
      "      B-PERSON      0.955     0.860     0.905       737\n",
      "      I-PERSON      0.926     0.912     0.919      2696\n",
      "       B-PHONE      0.941     0.889     0.914        18\n",
      "       I-PHONE      0.972     1.000     0.986        70\n",
      "        B-TIME      0.949     0.794     0.865       209\n",
      "        I-TIME      0.956     0.892     0.923       415\n",
      "         B-URL      1.000     0.889     0.941        18\n",
      "         I-URL      1.000     0.984     0.992       255\n",
      "         B-ZIP      1.000     1.000     1.000         3\n",
      "\n",
      "     micro avg      0.911     0.832     0.870     10863\n",
      "     macro avg      0.853     0.774     0.810     10863\n",
      "  weighted avg      0.909     0.832     0.868     10863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, X_test, y, y_test = train_test_split(X_data, y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True,\n",
    "    model_filename=file_name+\"-pos.model0\"\n",
    ")\n",
    "crf.fit(X, y);\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "y_pred = crf.predict(X_test)\n",
    "e=metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n",
    "print(e)\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X_data[0]\n",
    "#del y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "crf2 = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True,\n",
    "    model_filename=file_name+\".model\"\n",
    ")\n",
    "crf2.fit(X_data, y_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"train.data\", \"wb\") as dill_file:\n",
    " dill.dump(datatofile, dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate\n",
    "\"\"\"\n",
    "import dill\n",
    "with open(\"datatrain.data\", \"wb\") as dill_file:\n",
    " dill.dump(datatofile, dill_file)\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, average='macro') \n",
    "\n",
    "scores = cross_validate(crf, X, y, scoring=f1_scorer, cv=5)\n",
    "# save data\n",
    "print(scores)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
